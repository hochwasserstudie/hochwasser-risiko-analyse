{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "f4ac-29OldWf",
        "outputId": "e614d6d3-abf8-44fe-cce7-167655baa06c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b36c5db64b7d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#NLP Pakekte importieren (Linguistische Sachen)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#===============================================================================\n",
        "# PAKETE LADEN\n",
        "#===============================================================================\n",
        "\n",
        "#!pip uninstall -y scipy\n",
        "#!pip uninstall -y gensim\n",
        "#!pip install scipy == 1.10.1\n",
        "#!pip install gensim\n",
        "\n",
        "#Schneidjerberg\n",
        "import re\n",
        "import os\n",
        "import spacy\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "\n",
        "#NLP Pakekte importieren (Linguistische Sachen)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "#Importiert Befehle, die für die Arbeit mit Word wichtig sind\n",
        "!pip install python-docx\n",
        "from docx import Document\n",
        "import shutil\n",
        "\n",
        "#Lemmatisierung\n",
        "!pip install spacy\n",
        "!python -m spacy download de_core_news_sm\n",
        "\n",
        "#Tokenisierung\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt', download_dir='/usr/local/share/nltk_data')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "#Importiert Befehle, welche mit dem Drive zu tun haben\n",
        "from google.colab import drive\n",
        "\n",
        "#Topic Model\n",
        "!pip install pyLDAvis\n",
        "from gensim import corpora\n",
        "import pyLDAvis.gensim_models\n",
        "from gensim.models import TfidfModel, LdaMulticore\n",
        "from gensim.similarities import MatrixSimilarity\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "#===============================================================================\n",
        "# VERBINDUNG ZUM GOOGLE DRIVE & ORDNERVERWALTUNG\n",
        "#===============================================================================\n",
        "\n",
        "#Stellt eine Verbindung zum Drive her\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Definiert die Variable \"Word-Dateien\", welche auf den Ordner \"Zeitungsartikel\"\n",
        "#  Pfad zu deinem Ordner auf Google Drive\n",
        "folder_path = '/content/drive/MyDrive/Zeitungsartikel_final'\n",
        "\n",
        "#===============================================================================\n",
        "# unnötige Txt-Dokumente löschen\n",
        "#===============================================================================\n",
        "\n",
        "# Funktion zum Löschen der .txt-Dateien im Ordner und seinen Unterordnern\n",
        "def delete_txt_files_in_directory(folder_path):\n",
        "    # Durchlaufe alle Dateien und Unterordner im angegebenen Ordner\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.txt'):  # Nur .txt-Dateien löschen\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                try:\n",
        "                    os.remove(file_path)  # Lösche die Datei\n",
        "                    #print(f\"Datei gelöscht: {file_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Fehler beim Löschen der Datei {file_path}: {e}\")\n",
        "\n",
        "delete_txt_files_in_directory(folder_path)\n",
        "\n",
        "#===============================================================================\n",
        "# DATEIEN UMBENENNEN (Leerzeichen entfernen)\n",
        "#===============================================================================\n",
        "# Funktion zur Bereinigung der Dateinamen\n",
        "def bereinige_dateinamen(folder_path):\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise FileNotFoundError(\"Der Ordner wurde nicht gefunden.\")\n",
        "\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for datei in files:\n",
        "            if datei.lower().endswith(\".docx\"):\n",
        "                alter_name = os.path.join(root, datei)\n",
        "                neuer_name = os.path.join(root, datei.replace(\" \", \"_\").lower())\n",
        "\n",
        "                try:\n",
        "                    os.rename(alter_name, neuer_name)\n",
        "                    #print(f\"Datei umbenannt: {alter_name} -> {neuer_name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Fehler beim Umbenennen der Datei {datei}: {e}\")\n",
        "\n",
        "bereinige_dateinamen(folder_path)\n",
        "\n",
        "#===============================================================================\n",
        "#Umwandlung von Word in txt\n",
        "#===============================================================================\n",
        "# Funktion, um .docx-Dateien in .txt-Dateien umzuwandeln\n",
        "def docx_to_txt(docx_path, txt_path):\n",
        "    doc = Document(docx_path)\n",
        "    with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
        "        for para in doc.paragraphs:\n",
        "            txt_file.write(para.text + '\\n')\n",
        "\n",
        "# Alle .docx-Dateien im angegebenen Ordner und allen Unterordnern durchsuchen und umwandeln\n",
        "for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "    for filename in filenames:\n",
        "        if filename.endswith('.docx'):  # Jetzt nur auf 'filename' anwenden\n",
        "            docx_path = os.path.join(dirpath, filename)  # Voller Pfad zur .docx-Datei\n",
        "            txt_filename = filename.replace('.docx', '.txt')\n",
        "            txt_path = os.path.join(dirpath, txt_filename)  # Voller Pfad zur .txt-Datei\n",
        "\n",
        "            docx_to_txt(docx_path, txt_path)\n",
        "\n",
        "#===============================================================================\n",
        "#Dokumente systematisch einlesen + Reduzierung auf Textbody\n",
        "#===============================================================================\n",
        "# Funktion, die den Text zwischen 'Body' und 'Classification' extrahiert\n",
        "def extract_body_classification(text):\n",
        "    start_word = \"Body\"\n",
        "    end_word = \"Classification\"\n",
        "\n",
        "    # Suche nach dem Start- und Endpunkt des Textes\n",
        "    start_index = text.find(start_word)\n",
        "    end_index = text.find(end_word)\n",
        "\n",
        "    if start_index != -1 and end_index != -1 and start_index < end_index:\n",
        "        # Extrahiere den Text zwischen den beiden Wörtern\n",
        "        extracted_text = text[start_index + len(start_word):end_index].strip()\n",
        "        return extracted_text\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Hauptfunktion, die durch alle Unterordner und Dateien geht\n",
        "def process_txt_files(folder_path):\n",
        "    # Durchlaufe alle Dateien und Unterordner im angegebenen Ordner\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.txt'):  # Nur .txt-Dateien bearbeiten\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "\n",
        "                # Öffne die Datei zum Lesen\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()\n",
        "\n",
        "                # Extrahiere den Text zwischen 'Body' und 'Classification'\n",
        "                extracted_text = extract_body_classification(text)\n",
        "\n",
        "                if extracted_text:\n",
        "                    # Speichere den extrahierten Text zurück in die Datei\n",
        "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "                        file.write(extracted_text)\n",
        "                    #print(f\"Datei {filename} wurde bearbeitet und bereinigt.\")\n",
        "                else:\n",
        "                    print(f\"Kein Text zwischen 'Body' und 'Classification' in {filename} gefunden.\")\n",
        "\n",
        "# Starte die Verarbeitung\n",
        "process_txt_files(folder_path)\n",
        "\n",
        "#===============================================================================\n",
        "# DATEN BEREINIGEN: Tokenisierung\n",
        "#===============================================================================\n",
        "\n",
        "# Funktion zur Tokenisierung einer Datei\n",
        "def tokenize_text(text):\n",
        "    # Umwandlung in Kleinbuchstaben\n",
        "    text = text.lower()\n",
        "    # Entfernen von Sonderzeichen und Zahlen mit regulären Ausdrücken\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenisierung des Textes\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Funktion zur Verarbeitung aller .txt-Dateien in einem Ordner und dessen Unterordnern\n",
        "def process_files_in_directory(folder_path):\n",
        "    all_tokens = []\n",
        "\n",
        "    # Durchlaufe alle Dateien und Unterordner\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.txt'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Lese die Datei\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    text = f.read()\n",
        "                    # Tokenisiere den Text und füge die Tokens der Liste hinzu\n",
        "                    tokens = tokenize_text(text)\n",
        "                    all_tokens.extend(tokens)\n",
        "\n",
        "    return all_tokens\n",
        "\n",
        "# Beispiel: Verzeichnis, das verarbeitet werden soll\n",
        "tokens = process_files_in_directory(folder_path)\n",
        "\n",
        "#===============================================================================\n",
        "# DATEN BEREINIGEN: Lemmatisierung\n",
        "#===============================================================================\n",
        "# Lade das deutsche Sprachmodell von spaCy\n",
        "nlp = spacy.load('de_core_news_sm')\n",
        "\n",
        "# Funktion zur Lemmatisierung eines Textes\n",
        "def lemmatize_text(text):\n",
        "    # Verarbeite den Text mit spaCy\n",
        "    doc = nlp(text)\n",
        "    # Extrahiere die Lemmata der Tokens\n",
        "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
        "    return lemmatized_text\n",
        "\n",
        "# Funktion, die rekursiv alle .txt-Dateien im Ordner und seinen Unterordnern durchsucht\n",
        "def lemmatize_files_in_directory(directory):\n",
        "    # Durchlaufe alle Dateien und Unterordner im angegebenen Ordner\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.txt'):  # Wenn es sich um eine .txt-Datei handelt\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    # Öffne die Datei und lese den Inhalt\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        text = f.read()\n",
        "\n",
        "                    # Lemmatisiere den Text\n",
        "                    lemmatized_text = lemmatize_text(text)\n",
        "\n",
        "                    # Überschreibe die Originaldatei mit dem lemmatisierten Text\n",
        "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                        f.write(lemmatized_text)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Fehler beim Verarbeiten von {file_path}: {e}\")\n",
        "\n",
        "lemmatize_files_in_directory(folder_path)\n",
        "\n",
        "#===============================================================================\n",
        "# DATEN BEREINIGEN: Stopwörter\n",
        "#===============================================================================\n",
        "\n",
        "# Liste der Stopwörter (kann beliebig erweitert werden)\n",
        "stopwoerter = ['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm',\n",
        "               'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'dass',\n",
        "               'daß', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen',\n",
        "               'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein',\n",
        "               'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es',\n",
        "               'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier',\n",
        "               'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem',\n",
        "               'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines',\n",
        "               'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner',\n",
        "               'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem',\n",
        "               'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll',\n",
        "               'sollte', 'sondern', 'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von',\n",
        "               'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde',\n",
        "               'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen',\n",
        "               'werden', 'sein', 'haben', 'sagen', 'geben', 'kommen', 'können', 'stehen', 'müssen', 'gehen', 'sagen', 'vieler', 'schon', 'gut', 'der', 'ganz',\n",
        "               'seit', 'deshalb', 'besonders', ':', '--', 'davon', 'sehen', 'etwa', 'ja', 'dafür', 'erst', 'vielleicht', 'bringen', 'wer', 'hören', 'holen',\n",
        "               'fahren', 'liegen', 'mal', 'ab', 'allerdings','GesamtseitenPDF', '%','*','Gesamtseiten','PDF', 'Gesamtseiten-PDF', 'Original','sollen', 'mehr', 'Gesamtseiten--PDF', 'taz',\n",
        "               'welt', 'WELT', 'Zeit', 'Bild', 'zwei','drei', 'vier', 'erster', 'zweiter', 'dritter', 'ZEIT','gesamtseiten-pdf', '%', '*', 'Gesamtseiten', 'PDF', 'Gesamtseiten-PDF', 'Original', 'original', 'sollen', 'mehr', 'Gesamtseiten--PDF', 'taz',\n",
        "               'Köhler', 'köhler', 'angelina', 'Angelina', 'Fox', 'fox' 'Solomon', 'solomon','Jahr', 'jahr', 'groß', 'neu','immer','hoch','heute','weniger','zeigen','natürlich','dürfen','Teil','stellen','vergangen', 'groß',\n",
        "               'bleiben', 'stark','bereits','Tag','wegen','steigen','halten','letzter','weit','stellen','dürfen', 'mensch',\n",
        "\n",
        "               'eigen', 'tun', 'führen', 'wissen', 'deutlich', 'lassen', 'klein', 'Klein', 'finden', 'rund', 'alt', 'gerade', 'nehmen', 'ziehen', 'oft', 'dabei', 'gestern',\n",
        "               'mehrere', 'Teil', 'Dylan', 'Naomi', 'Schlotterbeck', 'dabei', 'Bickhardt', 'fast', 'Fuhrmann', 'Fischer', 'The', 'ZEIT', 'Zeit', 'zeit', 'zeit.', 'brauchen',\n",
        "               'kühne', 'tag', 'dylan', 'frau', 'wichtig', 'erklären', 's', 'seite', 'art', 'schlotterbeck', 'fischer', 'orth', 'fallen', 'aktuell', 'heißen', 'pro', 'schnell',\n",
        "               'erreichen', '1.', 'erster', 'heino', 'bild', 'aserbaidschan', 'bergkarabach', 'armeinen', 'aserbaidschanisch', 'haien', 'jersey', 'P.', 'Iran', 'Bickhardt','wasser','Wasser']\n",
        "\n",
        "def remove_stopwords_from_txt(folder_path, stopwoerter):\n",
        "\n",
        "    # Überprüfen, ob der Ordner existiert\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(\"Der angegebene Ordner existiert nicht.\")\n",
        "        return\n",
        "\n",
        "    # Alle Dateien im Ordner und Unterordnern durchlaufen\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            # Überprüfen, ob die Datei eine .txt-Datei ist\n",
        "            if file.endswith(\".txt\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "\n",
        "                # Datei öffnen und lesen\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    text = f.read()\n",
        "\n",
        "                # Wörter entfernen, die in der stopwoerter-Liste enthalten sind\n",
        "                text_words = text.split()\n",
        "                cleaned_words = [word for word in text_words if word.lower() not in stopwoerter]\n",
        "\n",
        "                # Bereinigten Text zusammenfügen\n",
        "                cleaned_text = \" \".join(cleaned_words)\n",
        "\n",
        "                # Datei mit bereinigtem Inhalt neu speichern\n",
        "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(cleaned_text)\n",
        "\n",
        "    print(\"Die Stopwörter wurden aus allen .txt-Dokumenten entfernt.\")\n",
        "\n",
        "remove_stopwords_from_txt(folder_path, stopwoerter)\n",
        "\n",
        "def remove_hyphens_from_txt(folder_path):\n",
        "\n",
        "    # Überprüfen, ob der Ordner existiert\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(\"Der angegebene Ordner existiert nicht.\")\n",
        "        return\n",
        "\n",
        "    # Alle Dateien im Ordner und Unterordnern durchlaufen\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            # Überprüfen, ob die Datei eine .txt-Datei ist\n",
        "            if file.endswith(\".txt\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "\n",
        "                # Datei öffnen und lesen\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    text = f.read()\n",
        "\n",
        "                # Sonderzeichen \"-\" entfernen\n",
        "                cleaned_text = text.replace(\"-\", \"\")\n",
        "\n",
        "                # Datei mit bereinigtem Inhalt neu speichern\n",
        "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(cleaned_text)\n",
        "\n",
        "    print(\"Das Sonderzeichen '-' wurde aus allen .txt-Dokumenten entfernt.\")\n",
        "\n",
        "remove_hyphens_from_txt(folder_path)\n",
        "\n",
        "#===============================================================================\n",
        "# Regulärer Ausdruck\n",
        "#===============================================================================\n",
        "\n",
        "# Liste der Risikobegriffe (Teilworte)\n",
        "risiko_keywords = [\"risik\", \"wahrscheinlich\", \"poten\", \"eintritt\", \"eingetreten\", \"ausmaß\", \"schad\", \"schäd\", \"erwart\", \"verletz\", \"schutz\", \"vulnerab\",\n",
        "                   \"katastroph\", \"natur\", \"bewältig\", \"anpass\", \"angepasst\", \"tot\", \"gestorben\", \"sterb\", \"ausgesetzt\", \"aussetz\", \"verlust\", \"verlier\", \"verlor\",\n",
        "                   \"fehler\", \"extrem\", \"knapp\", \"verschmutz\", \"klima\", \"bedroh\", \"schutz\", \"sozi\", \"technologi\", \"ökonom\",\"gesundheit\", \"umwelt\", \"maßnahme\",\n",
        "                   \"schuld\", \"fehler\", \"zukunft\", \"zukünft\", \"vorhersag\", \"einschätz\", \"unsicher\", \"gefahr\", \"gefähr\", \"individ\", \"privat\", \"politi\", \"bund\",\n",
        "                   \"region\", \"kommun\", \"lokal\", \"versicher\", \"prävent\",]\n",
        "\n",
        "\n",
        "def count_risk_tokens(text, risk_keywords):\n",
        "    #Zählt, wie oft Risikobegriffe im Text vorkommen\n",
        "    count = sum(len(re.findall(rf\"{keyword}\", text, re.IGNORECASE)) for keyword in risk_keywords)\n",
        "    return count\n",
        "\n",
        "def process_risk_documents_in_directory(folder_path, risk_keywords, min_risk_count=5):\n",
        "    total_files = 0\n",
        "    relevant_documents = []\n",
        "\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.txt'):\n",
        "                total_files += 1\n",
        "                file_path = os.path.join(root, file)\n",
        "\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    text = f.read()\n",
        "\n",
        "                risk_token_count = count_risk_tokens(text, risk_keywords)\n",
        "\n",
        "                if risk_token_count >= min_risk_count:\n",
        "                    relevant_documents.append((file_path, text))\n",
        "\n",
        "    print(f\"Gesamtzahl der .txt-Dateien: {total_files}\")\n",
        "    print(f\"Anzahl der relevanten Dokumente nach Filterung: {len(relevant_documents)}\")\n",
        "\n",
        "    return relevant_documents\n",
        "\n",
        "relevant_docs = process_risk_documents_in_directory(folder_path, risiko_keywords, min_risk_count=5)\n",
        "\n",
        "#===============================================================================\n",
        "#Bag of Words\n",
        "#===============================================================================\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Funktion zur Erstellung der Bag-of-Words-Darstellung\n",
        "def create_bag_of_words(documents):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(documents)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    df_bow = pd.DataFrame(X.toarray(), columns=feature_names)\n",
        "    return df_bow\n",
        "\n",
        "#===============================================================================\n",
        "#Topic Model\n",
        "#===============================================================================\n",
        "\n",
        "# Funktion zum Laden der Textdateien\n",
        "def load_documents_from_folder(folder_path):\n",
        "    documents = []\n",
        "    for subdir, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".txt\"):  # Falls deine Dateien .txt sind\n",
        "                file_path = os.path.join(subdir, file)\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    documents.append(f.read())\n",
        "    return documents\n",
        "\n",
        "# Lade die Dokumente\n",
        "documents = load_documents_from_folder(folder_path)\n",
        "\n",
        "processed_docs = [text.split() for text in documents]\n",
        "\n",
        "# Erstelle ein Dictionary und ein Korpus\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_docs]\n",
        "\n",
        "# Erstelle das LDA Modell\n",
        "lda_model = gensim.models.LdaMulticore(\n",
        "    corpus,\n",
        "    num_topics=4,\n",
        "    id2word=dictionary,\n",
        "    passes=50,\n",
        "    workers=2,\n",
        "    alpha=0.2,\n",
        "    eta=0.1,\n",
        "    iterations=800,\n",
        "    random_state=99\n",
        ")\n",
        "\n",
        "#==============================================================================\n",
        "# Topic Model Optimierung\n",
        "#===============================================================================\n",
        "\n",
        "# Kohärenz\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "\n",
        "#===============================================================================\n",
        "#Visualisierung Topic Model\n",
        "#===============================================================================\n",
        "\n",
        "# Visualisiere das LDA Modell\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "vis\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}